#!/usr/bin/python3

"""
Translation Agent.

Usage:
  translate text [options] [<blob>]
  translate model download <model>
  translate model list online
  translate model list local
  translate (-h | --help)
  translate --version

Options:
  -s <source>, --source <source>   Source language. [default: en]
  -t <target>, --target <target>   Target language. [default: fr]
  -e <engine>, --engine <engine>   Huggingface model (supersedes -s and -t).
  -h --help    Show this screen.
  --version    Show version.
"""

# This script performs offline translations using Huggingface transformers.
# If no translation model is specified, translates from english to french.

__author__ = "Richard Jarry"
__version__ = "0.0.1"

import os, sys, logging

from glob import glob
from docopt import docopt
from typing import List, Optional

from huggingface_hub import list_models

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def translate(text:List[str],model_name:str) -> List[str]:

    assert(len(model_name)>0)
    try:
      # Initialize the tokenizer and the model
      tokenizer = AutoTokenizer.from_pretrained(model_name,local_files_only=True)
      model = AutoModelForSeq2SeqLM.from_pretrained(model_name,local_files_only=True)

    except OSError: 
      logging.disable(logging.ERROR)
      print(f'Error while loading model "{model_name}".')
      print("Has it been correctly downloaded beforehand?")
      return ['']

    # Tokenize text
    tokenized_text = tokenizer(text,return_tensors='pt',padding=True)

    # Perform translation and decode the output 
    translation = model.generate(**tokenized_text) 
    translated = tokenizer.batch_decode(translation, skip_special_tokens=True)

    return translated

def download(model_name:str) -> None:

    assert(len(model_name)>0)
    try:

      logging.disable(logging.ERROR)
      _ = AutoTokenizer.from_pretrained(model_name,force_download=True)
      _ = AutoModelForSeq2SeqLM.from_pretrained(model_name,force_download=True)

    except OSError:
      print(f'Error while fetching model "{model_name}".')
      print("Please check its availability on the Huggingface Hub.")
      return

def list_online() -> None:
    return [m.modelId for m in list_models() if m.pipeline_tag == 'translation']

def list_local(cachedir:Optional[str]=None) -> None:

    if cachedir is None: 
      cachedir = os.path.expanduser('~')
      cachedir += "/.cache/huggingface/transformers/"

    filenames = glob(cachedir+'*.json')
    result = []

    for filename in filenames:
      content = open(filename,'r').read()
      current = content.split('/')[3:5]
      current = '/'.join(current)
      result = result + [current]

    return result
    
if __name__ == '__main__':

    args = docopt(__doc__, version=__version__)

    source = args['--source']
    target = args['--target']
    engine = args['--engine']

    blob = args['<blob>']

    if args['text']:
      default_engine = f'Helsinki-NLP/opus-mt-{source}-{target}'
      engine = default_engine if not engine else engine
      lines = [line for line in sys.stdin] if not blob else [blob]
      result = translate(lines, engine)
      print('\n'.join(result))

    if args['download']:
      download(args['<model>'])

    if args['list'] and args['online']:
      print('\n'.join(list_online()))

    if args['list'] and args['local']:
      print('\n'.join(list_local()))
